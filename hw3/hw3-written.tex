\documentclass[12pt,a4paper]{article}

\usepackage{amsmath}
\usepackage{booktabs}

\usepackage{enumitem}
\setlist[enumerate]{noitemsep, topsep=0pt}
\setlist[itemize]{noitemsep, topsep=0pt}

\usepackage{fullpage}
\usepackage{multirow}
\usepackage{tabularx}

\usepackage{tikz}
\usepackage{tikz-qtree}

\setlength\parindent{0pt}

\begin{document}
\pagestyle{empty}

\textbf{CS 6320.002: Natural Language Processing} \\
\textbf{Fall 2020} \\

\textbf{Homework 3 --- 45 points} \\
\textbf{Issued 04 Oct. 2021} \\
\textbf{Due 11:59pm 18 Oct. 2021} \\

\textbf{Deliverables:} Answers can be typed directly into Gradescope. LaTeX can be hand-typed or generated using Mathpix Snip. See the assignment guide for more details.

\vspace{\baselineskip}

\textbf{What does it mean to ``show your work?"} Write out the math step-by-step; we should be able to clearly follow your reasoning from one step to another. (You can combine ``obvious" steps like simplifying fractions or doing basic arithmetic.) The point of showing your work is twofold: to get partial credit if your answer is incorrect, and to show us that you worked the problem yourself and understand it. We will deduct points if steps are missing.

\section{Constituent Parsing}

The problems in this section are based on the material covered in Week 6.

\subsection{Context Free Grammars --- 5 points}

 Suppose we have the following CFG:

\begin{center}
\begin{tabular}{l c l}
S & $\rightarrow$ & NP VP \\
VP & $\rightarrow$ & VB NP \\
NP & $\rightarrow$ & NP and NP \\
NP & $\rightarrow$ & JJ NP \\
NP & $\rightarrow$ & NN \\
JJ & $\rightarrow$ & black \\
NN & $\rightarrow$ & I \\
NN & $\rightarrow$ & cats \\
NN & $\rightarrow$ & dogs \\
VB & $\rightarrow$ & like \\
\end{tabular}
\end{center}

How many possible parse trees are there for the sentence ``I like black cats and dogs"? Show the derivations for each possible parse, ie. the rules used and the order they are used in.

\subsection{Context Free Grammars --- 5 points}

Suppose we have the following CFG:

\begin{center}
\begin{tabular}{l c l}
S & $\rightarrow$ & NP VP \\
NP & $\rightarrow$ & DT NN \\
NP & $\rightarrow$ & DT NNS \\
VP & $\rightarrow$ & VB NP \\
DT & $\rightarrow$ & the \\
NN & $\rightarrow$ & cat \\
NN & $\rightarrow$ & dog \\
NNS & $\rightarrow$ & cats \\
NNS & $\rightarrow$ & dogs \\
VB & $\rightarrow$ & see \\
VB & $\rightarrow$ & sees\\
\end{tabular}
\end{center}

This grammar can generate ungrammatical sentences, such as ``the dog see the cat" and ``the dogs sees the cat." What rule(s) would you need to add or change so that the grammar generates only grammatical sentences? List all the rules in your new grammar.

\subsection{Probabilistic Context Free Grammars --- 5 points}

Suppose we have the following PCFG:

\begin{center}
\begin{tabular}{l c l l}
S & $\rightarrow$ & NP VP & 1.0 \\
NP & $\rightarrow$ & DT JJ NN & 1.0 \\
VP & $\rightarrow$ & VB & 1.0 \\
DT & $\rightarrow$ & the & 1.0 \\
JJ & $\rightarrow$ & happy & 0.5 \\
JJ & $\rightarrow$ & $\epsilon$ & 0.5 \\
NN & $\rightarrow$ & cat & 1.0 \\
VB & $\rightarrow$ & sat & 1.0 \\
\end{tabular}
\end{center}

Convert this PCFG into Chomsky normal form. If you need new non-terminal symbols, use $X$, $Y$, and $Z$. Make sure you do not change the probabilities of sentences generated by this grammar.

\subsection{Probabilistic Context Free Grammars --- 5 points}

Suppose we have the following PCFG:

\begin{center}
\begin{tabular}{l c l l}
S & $\rightarrow$ & NP VP & 1.0 \\
NP & $\rightarrow$ & DT NN & 1.0 \\
VP & $\rightarrow$ & MD VP & 0.5 \\
VP & $\rightarrow$ & VB NP & 0.5 \\
DT & $\rightarrow$ & the & 1.0 \\
NN & $\rightarrow$ & cat & 0.5 \\
NN & $\rightarrow$ & can & 0.5 \\
MD & $\rightarrow$ & can & 1.0 \\
VB & $\rightarrow$ & see & 1.0 \\
\end{tabular}
\end{center}

Use the CKY algorithm to find the highest probability parse of the sentence ``The cat can see the can". Show your work, ie. the dynamic programming table. (Note that CKY for probabilistic PCFGs uses a three-dimensional dynamic programming table. Instead of trying to format a 3D table, simply use a 2D table, as in CKY for non-probabilistic CFGs, and show both the rule(s) being used and the associated probability in each table entry, as we do in lecture.)

\section{Dependency Parsing}

The problems in this section are based on the material covered in Week 7.

\vspace{\baselineskip}

Consider the sentence ``I think I like cats."

\subsection{5 points}

Parse this sentence by hand and list the dependency relations. You can use arrows, as we do in lecture, or simply list pairs of (head, dependent).

\subsection{5 points}

Parse this sentence using shift-reduce parsing, the arc-standard transition system, and no arc labels. You can assume you have a perfet oracle that tells you the correct action to take at each time step. Show the stack, buffer, action, and relation set at each time step (you can use a table, as we do in lecture, or simply show a series of lists).

\subsection{5 points}

Parse this sentence again, this time using the arc-eager transition system. Show your work as before.

\section{Machine Translation}

The problems in this section are based on the material covered in Week 8.

\vspace{\baselineskip}

Suppose we have a training corpus of three sentences:

\begin{center}
\begin{tabular}{r l l c r l l c r l l}
1. & the & dog & & 2. & little & dog & & 3. & black & dog \\
& le & chien & & & petit & chien & & & chien & noir \\
\end{tabular}
\end{center}

Now suppose we have a word-level statistical machine translation model with the parameters $q(j|i, l, m)$ and $t(f_i | e_j)$ initialized uniformly using this corpus.

\subsection{5 points}

Perform one iteration of expectation maximization to update the parameters using the training corpus. State the values of the $q$'s and $t$'s at the end of this iteration, plus the intermediate values you calculated for $p(a_i = j | f_i, e_j)$. You can simply state the values; no need to show the arithmetic.

\subsection{5 points}

Perform a second iteration of expectation maximization to update the parameters again. As before, state the values of the $q$'s and $t$'s at the end of this iteration, plus the intermediate values you calculated for $p(a_i = j | f_i, e_j)$.

\end{document}